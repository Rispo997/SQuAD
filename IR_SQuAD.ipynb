{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IR_SQuAD.ipynb","provenance":[],"collapsed_sections":["XPGWhEKBft7A","lBpGzlzIzqSM","cMhQIX51uwuP","gVICAMMz0nkl","lOx947m2j1-_","-5GGKXF_f6zt","aIILPIeGOnaF","wkwzlC6669ii","7QYo4Xr17FmD","b-yMofLP7JXy","tFYzxMTVJb9e","co7rg1H2JfPJ","XwyVzQ1qJUQu","Xvqxd1z-PKAN","W2OSrg2DDnqI","AIrl244VnaTa","kknhKzxhGgam","6V1R3YUhGeTX","1O0EXbotzZav"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XPGWhEKBft7A"},"source":["## Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM3HH3kzfxYv","executionInfo":{"status":"ok","timestamp":1622664510714,"user_tz":-120,"elapsed":24074,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"103f19e8-1ccd-40ab-9712-4ae94cbbc802"},"source":["!pip install --upgrade spacy\n","!python -m spacy download en_core_web_trf\n","!pip3 install pickle5"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.0.6)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.3)\n","Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n","Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n","Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.4)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n","Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.5.2)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n","2021-06-02 20:08:18.511451: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Requirement already satisfied: en-core-web-trf==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl#egg=en_core_web_trf==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: spacy-transformers<1.1.0,>=1.0.0rc4 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (1.0.2)\n","Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (3.0.6)\n","Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.8.1+cu101)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.4.1)\n","Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.8.3)\n","Requirement already satisfied: transformers<4.6.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (4.5.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (20.9)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.5.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.8.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.4.1)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.7.4.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (57.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.4)\n","Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.7.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.11.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.41.1)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.19.5)\n","Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (4.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.7)\n","Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.4)\n","Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.4.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.1)\n","Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_trf')\n","Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f64auGCQnv8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622664786847,"user_tz":-120,"elapsed":296,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"f0baf6be-9c1b-498e-8d7f-3b09b8fe7500"},"source":["import numpy as np\n","import json\n","import pandas as pd\n","import pickle5 as pickle\n","import math\n","import time\n","from typing import *\n","import re\n","import tqdm.autonotebook as tqdm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  if __name__ == '__main__':\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lBpGzlzIzqSM"},"source":["## Mount drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xf4oOOOZoE3X","executionInfo":{"status":"ok","timestamp":1622664788641,"user_tz":-120,"elapsed":232,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"7da7705d-987c-4e57-a5f3-0e2fa318aeeb"},"source":["from google.colab import drive, files\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_flSKlyhe8Xx"},"source":["##Dataset"]},{"cell_type":"code","metadata":{"id":"aj5ekQsbsnQv"},"source":["data_path = '/content/gdrive/My Drive/NLP/Project/SQUAD MATERIAL/'\n","models_path = '/content/gdrive/My Drive/NLP/Project/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMhQIX51uwuP"},"source":["### Load data"]},{"cell_type":"code","metadata":{"id":"GkxGyhT2Lkf6","executionInfo":{"status":"ok","timestamp":1622664821873,"user_tz":-120,"elapsed":30023,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["with open(data_path + 'doc_dic2.pkl', 'rb') as handle:\n","  preprocessed_documents = pickle.load(handle)\n","with open(data_path + 'qst_dic2.pkl', 'rb') as handle:\n","  preprocessed_questions = pickle.load(handle)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVICAMMz0nkl"},"source":["### Split dataset"]},{"cell_type":"code","metadata":{"id":"RhUPPROwp7Ii","executionInfo":{"status":"ok","timestamp":1622664822226,"user_tz":-120,"elapsed":367,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["#load the partition of the documents in the training/validaton/test sets\n","titles_split = pickle.load(open(data_path + 'titles_split.pkl', 'rb'))\n","paragraphs_as_docs = True"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wowZjePN6tDF","executionInfo":{"status":"ok","timestamp":1622664827974,"user_tz":-120,"elapsed":5751,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["def split_set(dataset, split_part):\n","  index = math.floor((1 - split_part) * len(dataset))\n","  return dataset[:index], dataset[index:]\n","\n","def is_title_in_title_set(title, title_set):\n","  return re.sub(r'_\\d+$', '', title) in title_set\n","\n","#split the dataset according to the partiotioning described in titles_split\n","def split_dataset_by_titles(dataset, titles_split):\n","  res = dict()\n","  for key in titles_split.keys():\n","    res[key] = []\n","  for elem in dataset:\n","    for key,title_set in titles_split.items():\n","      if is_title_in_title_set(elem[0], title_set):\n","        res[key].append(elem)\n","  return res['train'], res['validation'], res['test']\n","\n","#selecting specific tokens (no punctuation and stopwords) in a certain form (plain text, lemma, ner, etc.) specified by the parameter 'token_form' of question and documents \n","def build_dataset(docs, questions, token_form, map=str.lower, split_paragraphs=False):\n","  \n","  map = (lambda x:x) if map is None else map\n","  if split_paragraphs:\n","    documents = [\n","      (f'{title}_{par_id}', [\n","        map(token[token_form])\n","        for token in paragraph if not (token['is_punct'] or token['is_stop'])\n","      ])\n","      for title, paragraphs in docs.items()\n","      for par_id, paragraph in paragraphs.items()\n","    ]\n","    \n","    questions = [\n","      (f'{title}_{par_id}', [\n","        [\n","          map(token[token_form])\n","          for token in question if not (token['is_punct'] or token['is_stop'])\n","        ]\n","        for question in qsts_paragraph.values()\n","      ])\n","      for title, qst_paragraphs in questions.items()\n","      for par_id, qsts_paragraph in qst_paragraphs.items()\n","    ]\n","  else:\n","    documents = [\n","      (title, [ \n","          map(token[token_form])\n","          for par_id, paragraph in paragraphs.items()\n","          for token in paragraph if not (token['is_punct'] or token['is_stop'])\n","      ])\n","      for title, paragraphs in docs.items()\n","    ]\n","    \n","    questions = [\n","      (title, [\n","        [\n","            map(token[token_form])\n","            for token in question if not (token['is_punct'] or token['is_stop'])\n","        ]\n","        for par_id, qst_paragraph in qst_paragraphs.items()\n","        for question in qst_paragraph.values()\n","        ])\n","      for title, qst_paragraphs in questions.items()\n","    ]\n","\n","  return documents, questions\n","\n","# Build datasets\n","dataset_documents, dataset_questions = build_dataset(\n","  preprocessed_documents,\n","  preprocessed_questions,\n","  token_form='lemma_',\n","  split_paragraphs=paragraphs_as_docs\n",")\n","\n","training_docs, validation_docs, test_docs = split_dataset_by_titles(dataset_documents, titles_split)\n","training_questions, validation_questions, test_questions = split_dataset_by_titles(dataset_questions, titles_split)\n","\n","_, dataset_questions_ner = build_dataset(\n","  preprocessed_documents,\n","  preprocessed_questions,\n","  token_form='ent_type',\n","  map=None,\n","  split_paragraphs=paragraphs_as_docs\n",")\n","\n","training_questions_ner, validation_questions_ner, test_questions_ner = split_dataset_by_titles(dataset_questions_ner, titles_split)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNVXOX0cevyB"},"source":["##Models"]},{"cell_type":"code","metadata":{"id":"1A18eO_aSaJO","executionInfo":{"status":"ok","timestamp":1622665064286,"user_tz":-120,"elapsed":6,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["from collections import defaultdict\n","from gensim import corpora\n","from gensim.models import LsiModel\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.utils import ClippedCorpus\n","from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOx947m2j1-_"},"source":["###Functions"]},{"cell_type":"code","metadata":{"id":"X147gszI38BU","executionInfo":{"status":"ok","timestamp":1622665064285,"user_tz":-120,"elapsed":725,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["#infer vector from Doc2Vec model\n","def get_Doc2Vec_vector(model, text, **kwargs):\n","  return model.infer_vector(text, epochs=kwargs.get('doc2vec_infer_epochs', 200))\n","\n","#infer vector from LSA model\n","def get_LSA_vector(model, text, **kwargs):\n","  return [v for d,v in model[kwargs['dictionary'].doc2bow(text)]]\n","\n","#compute similarity using sklearn library\n","def cosine_similarity(question_vector, doc_vector):\n","  return sk_cosine_similarity([question_vector], [doc_vector]).item()\n","\n","def sort_dict_by_value(dictionary, reverse=True):\n","  return {k: v for k, v in sorted(dictionary.items(), reverse=reverse, key=lambda item: item[1])}\n","\n","#check if one of the two vectors has dimension 0\n","def not_null(vector1, vector2):\n","  if type(vector1) != tuple:\n","    return len(vector1)*len(vector2)\n","  for i in range(len(vector1)):\n","    if not len(vector1[i])*len(vector2[i]):\n","      return False\n","  return True\n","\n","def is_word_in_doc(word, doc):\n","  return word in doc\n","\n","def is_question_word_in_doc(question, doc):\n","  for word in question:\n","    if is_word_in_doc(word, doc):\n","      return True\n","  return False\n","\n","#select the tokens inside the param text that are tagged with a NER tag\n","def select_ner_tokens(text, ner_tagged_text):\n","  entity_tokens = []\n","  for token, ner_tag in zip(text, ner_tagged_text):\n","    if ner_tag != 0:\n","      entity_tokens.append(token)\n","  return entity_tokens\n","\n","#compute with the 'models' the vectors of the 'documents' using the functions 'get_vector_functions'\n","def compute_documents_vectors(models, get_vector_functions, documents, **kwargs):\n","  doc_vectors = []\n","  model_scores = []\n","  for i,model in enumerate(models):\n","    doc_vectors.append(dict())\n","    #compute document vectors\n","    for title,doc in documents:\n","      doc_vectors[i][title] = (doc, get_vector_functions[i](model, doc, **kwargs))\n","  return doc_vectors\n","\n","#compute the similarity for each of the doc-question pairs\n","def compute_scores_by_vectors(doc_vectors, question_vector):\n","  scores = dict()\n","  #for title,(_,doc_vector) in tqdm.tqdm(doc_vectors.items()):\n","  for title,(_,doc_vector) in doc_vectors.items():\n","    #compute the score according to the cosine similarity received as parameter \n","    scores[title] = cosine_similarity(question_vector, doc_vector) if not_null(question_vector, doc_vector) else 0\n","  return scores\n","\n","#return the documents with the scores, sorted by the score\n","def compute_documents_ranking(models, get_vector_functions, doc_vectors, question, question_ner=None, score_normalization=True, exact_match_bonus=0, ner_exact_match_bonus=0, **kwargs):\n","  #definition of variables\n","  model_scores = []\n","  em_bonus_counter = 0\n","  ner_em_bonus_counter = 0\n","  \n","  #compute question vector and score between question and docs\n","  for i,model in enumerate(models):\n","    question_vector = get_vector_functions[i](model, question, **kwargs)\n","    model_scores.append(compute_scores_by_vectors(doc_vectors[i], question_vector))   \n","  #perform the normalization of the scores\n","  if score_normalization:\n","    for i in range(len(model_scores)):\n","      model_scores[i] = normalize_dict_values(model_scores[i])\n","\n","  #average scores if more models are passed\n","  scores = dict()\n","  for title in model_scores[0].keys():\n","    score_sum = 0\n","    for i in range(len(model_scores)):\n","      score_sum += model_scores[i][title]\n","    scores[title] = score_sum / len(model_scores)\n","\n","  #if the question vector is null, then all the scores will be equal to 0. If this condition doesn't hold compute the exact match's bonuses\n","  if not (min(scores.values()) == 0 and max(scores.values()) == 0):\n","    #compute exact match bonus\n","    if exact_match_bonus or ner_exact_match_bonus:\n","      #select tokens in the question with a NER tag assigned \n","      if question_ner and ner_exact_match_bonus:\n","        ner_tokens = select_ner_tokens(question, question_ner)\n","      for title,(doc,_) in doc_vectors[0].items():\n","        #add score bonus based on exact match if the doc contains the the words of the question\n","        keyword_present = is_question_word_in_doc(question, doc) if exact_match_bonus else False\n","        ner_keyword_present = is_question_word_in_doc(ner_tokens, doc) if question_ner and ner_exact_match_bonus else False\n","        #add exact match bonuses, if present, without exceed 1\n","        scores[title] = scores[title] + (exact_match_bonus * keyword_present) + (ner_exact_match_bonus * ner_keyword_present)\n","        #count how many pairs question-docs are affected by the exact match bonus\n","        if keyword_present:\n","          em_bonus_counter += 1\n","        if ner_keyword_present:\n","          ner_em_bonus_counter += 1\n","  #else:\n","    #print(\"Question: \", question, \"\\n\")\n","  ranked_docs = sort_dict_by_value(normalize_dict_values(scores))\n","  return ranked_docs, em_bonus_counter, ner_em_bonus_counter\n","\n","def normalize_dict_values(dictionary):\n","  #if all the values are not equal, perform the normalization\n","  #find min and max\n","  min_value = min(dictionary.values())\n","  max_value = max(dictionary.values())\n","  if min_value != max_value: \n","    #normalize: (value - min) / max - min\n","    for k,v in dictionary.items():\n","      dictionary[k] = (dictionary[k] - min_value) / (max_value - min_value)\n","  else:\n","    dictionary = {k:1 for k,_ in dictionary.items()}\n","  return dictionary\n","\n","#return the results of the metrics for a specific model (or a set of models combined together)\n","def evaluate_model(models, get_vector_functions, documents, questions, questions_ner=None, score_normalization=True, exact_match_bonus=0, ner_exact_match_bonus=0, **kwargs):\n","  #definition of variables\n","  similarity_score = []\n","  ranking_score = []\n","  positions = []\n","  target_doc_first = 0\n","  num_questions = 0\n","  \n","  #var used for debugging and for understaind better some behaviours\n","  em_bonus_counter = 0\n","  ner_em_bonus_counter = 0\n","  non_valid_samples = 0\n","  \n","  #compute the document vectors\n","  doc_vectors = compute_documents_vectors(models, get_vector_functions, documents, **kwargs)\n","\n","  #compute the doc-question scores\n","  ner_tagged_question = None\n","  #for (target_doc, doc_questions) in tqdm.tqdm(questions):\n","  for (target_doc, doc_questions) in questions:\n","    #check if EM_NER_bonus is enabled\n","    if questions_ner and ner_em_bonus:\n","      doc_questions_ner = [doc_questions_ner for doc_title,doc_questions_ner in questions_ner if doc_title==target_doc][0]\n","    #for each questions compute the ranking of the docs and update the metrics\n","    for i,question in enumerate(doc_questions):\n","      if questions_ner and ner_em_bonus:\n","        ner_tagged_question = doc_questions_ner[i]\n","      num_questions += 1\n","      #compute documents ranking\n","      ranked_docs, exact_match_bonuses, ner_exact_match_bonuses = compute_documents_ranking(models, get_vector_functions, doc_vectors, question, ner_tagged_question, score_normalization, exact_match_bonus, ner_exact_match_bonus, **kwargs)\n","      #update a counter to understand how many doc-questions paris are affected by the bonuses\n","      em_bonus_counter += exact_match_bonuses\n","      ner_em_bonus_counter += ner_exact_match_bonuses\n","\n","      #When testing paragraph serach knowing the target document, removing the paragraphs of the others paragraphs \n","      if kwargs.get('rank_paragraph_from_doc', False):\n","        ranked_docs = sort_dict_by_value({k:v for k,v in ranked_docs.items() if k[:k.rfind('-')] == target_doc[:target_doc.rfind('-')]})\n","\n","      #some questions in the dataset are mapped in a null vector: in that case the documents' scores are all equal to 0 \n","      if min(ranked_docs.values()) == max(ranked_docs.values()): \n","        #just a counte for debugging\n","        non_valid_samples += 1\n","      else:\n","        #compute metrics\n","        similarity_score.append(ranked_docs[target_doc])\n","        docs_positions = {k:(i+1) for i,(k,v) in enumerate(ranked_docs.items())}\n","        ranking_score.append(1/docs_positions[target_doc])\n","        positions.append(docs_positions[target_doc])\n","        if (docs_positions[target_doc] == 1):\n","          target_doc_first += 1      \n","\n","  return similarity_score, ranking_score, positions, target_doc_first/num_questions, non_valid_samples, (em_bonus_counter, ner_em_bonus_counter, len(documents*num_questions))\n","\n","def print_score(model_name, similarity_score, ranking_score, position_score, first_score, non_valid_samples, em_multiplier_counter):\n","  print(model_name, \":\\nSimilarity score: \", np.mean(similarity_score), \"\\nAverage position: \", np.mean(position_score), \n","        \"\\nStandard deviation of positions: \", np.std(position_score), \n","        \"\\nMediana, First 75%/90%/98% documents' position: : \", np.median(position_score), \", \", np.quantile(position_score, [0.75, 0.90, 0.98]), \"\\nRanking score: \", np.mean(ranking_score), \"\\nTarget doc as first: \", first_score, \"\\nNon valid test samples: \", non_valid_samples, \"\\nExact match bonus (done, total):\", em_multiplier_counter, \"\\n\")"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5GGKXF_f6zt"},"source":["###LSA model"]},{"cell_type":"markdown","metadata":{"id":"aIILPIeGOnaF"},"source":["####Preparation corpus, definition of the dictionary"]},{"cell_type":"code","metadata":{"id":"RESXrjMeeZ8z"},"source":["FREQUENCY_TRESHOLD = 1\n","\n","def remove_infrequent_words(corpus, frequency_threshold):\n","  # Count word frequencies\n","  frequency = defaultdict(int)\n","  for doc in corpus:\n","    for token in doc:\n","      frequency[token] += 1\n","  #Select words with frequence > N\n","  return [[token for token in doc if frequency[token] > frequency_threshold] for doc in corpus]\n","\n","def compute_dictionary_and_bow(dataset_docs):\n","  #Removing unfrequent words, initializing the dictionary and construct the bag of words of the document\n","  processed_corpus = remove_infrequent_words([doc for title,doc in dataset_docs], FREQUENCY_TRESHOLD)\n","  dictionary = corpora.Dictionary(processed_corpus)\n","  id2word = {v:k for k,v in dictionary.token2id.items()}\n","  bow_documents = [dictionary.doc2bow(doc) for doc in processed_corpus]\n","  return dictionary, id2word, bow_documents\n","\n","dictionary, id2word, bow_documents = compute_dictionary_and_bow(training_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkwzlC6669ii"},"source":["####Search for the best parameter"]},{"cell_type":"code","metadata":{"id":"8PtnZ4Ykebgg"},"source":["lsa_models = dict()\n","for latent_dimension in range(50, 375, 50):\n","  lsa_models[latent_dimension] = LsiModel(bow_documents, num_topics=latent_dimension, id2word=id2word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ckx3r1SpC_Vk"},"source":["for dim in range(100, 375, 50):\n","  print_score(\"LSA model (topic=\" + str(dim) + \")\", *evaluate_model([lsa_models[dim]], [get_LSA_vector], validation_docs, validation_questions, score_normalization=False, dictionary=dictionary))  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QYo4Xr17FmD"},"source":["#### Best model"]},{"cell_type":"code","metadata":{"id":"BH9q6yKmw1yY"},"source":["best_lsa_param = 350"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSGazVVElEr4"},"source":["dictionary, id2word, bow_documents = compute_dictionary_and_bow(training_docs+validation_docs)\n","best_lsa_model = LsiModel(bow_documents, num_topics=best_lsa_param, id2word=id2word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgJemPm6Ab4r"},"source":["best_lsa_model.save(models_path + \"lsa_model.lsa\")\n","dictionary.save(models_path + \"lsa_model_dictionary.dic\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-yMofLP7JXy"},"source":["###Doc2Vec model"]},{"cell_type":"markdown","metadata":{"id":"tFYzxMTVJb9e"},"source":["####Dataset preparation"]},{"cell_type":"code","metadata":{"id":"257qUcT3CGrA"},"source":["vdoc_train = [ TaggedDocument(words=doc, tags=[title]) for title, doc in training_docs ]\n","vdoc_val   = [ TaggedDocument(words=doc, tags=[title]) for title, doc in validation_docs ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"co7rg1H2JfPJ"},"source":["####Hyperparameters"]},{"cell_type":"code","metadata":{"id":"gU01aFCttG-m"},"source":["max_epochs = 100   #@param { type: \"number\" }\n","val_epochs = 150   #@param { type: \"number\" }\n","vec_size = 100     #@param { type: \"number\" }\n","alpha = 0.025      #@param { type: \"number\" }\n","print_every = 1    #@param { type: \"number\" }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwyVzQ1qJUQu"},"source":["####Search the best hyperparameters"]},{"cell_type":"code","metadata":{"id":"TSRsz1w_DFFW"},"source":["doc2vec_model = Doc2Vec(\n","  vector_size=vec_size,\n","  alpha=alpha,\n","  min_alpha=0.00025,\n","  min_count=1,\n","  dm=1,\n","  epochs=1,\n","  workers=8\n",")\n","\n","doc2vec_model.build_vocab(vdoc_train)\n","\n","for epoch in range(max_epochs):\n","    start_time = time.time()   \n","    doc2vec_model.train(\n","        vdoc_train,\n","        total_examples=doc2vec_model.corpus_count,\n","        epochs=doc2vec_model.epochs,\n","        queue_factor=16\n","    )\n","    if print_every and (epoch + 1) % print_every == 0:\n","      print_score(\"Doc2Vec model, epoch \" + str(epoch) + \":\", *evaluate_model([doc2vec_model], [get_Doc2Vec_vector], validation_docs, validation_questions, score_normalization=False))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0T1-CvyWPWGt"},"source":["## One cell training\n","doc2vec_model = Doc2Vec(\n","  vector_size=100,\n","  alpha=0.025,\n","  min_alpha=0.00025,\n","  min_count=1,\n","  dm=1,\n","  epochs=26,\n","  workers=8\n",")\n","doc2vec_model.build_vocab(vdoc_train)\n","doc2vec_model.train(\n","  vdoc_train,\n","  total_examples=doc2vec_model.corpus_count,\n","  epochs=doc2vec_model.epochs,\n","  queue_factor=16\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bn2q0-6yQeJ2","executionInfo":{"elapsed":243126,"status":"ok","timestamp":1621972087300,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"},"user_tz":-120},"outputId":"2a58a4e3-17e5-41ee-eb3a-4abfec82116c"},"source":["print_score(\"Doc2Vec model, epoch \" + str(doc2vec_model.epochs), *evaluate_model([doc2vec_model], [get_Doc2Vec_vector], validation_docs, validation_questions, score_normalization=False))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Doc2Vec model, epoch 26 :\n","Similarity score:  0.8455138808374248 \n","Average position:  6.520539013257987 \n","Standard deviation of positions:  9.77978910580268 \n","Mediana, First 75%/90%/98% documents' position: :  2.0 ,  [ 7. 21. 39.] \n","Ranking score:  0.583815509652554 \n","Target doc as first:  0.46815909584872856 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (0, 0, 404888) \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xvqxd1z-PKAN"},"source":["####Best model"]},{"cell_type":"code","metadata":{"id":"Wxqbrd_dgmU9"},"source":["## One cell training\n","best_doc2vec_model = Doc2Vec(\n","  vector_size=100,\n","  alpha=0.025,\n","  min_alpha=0.00025,\n","  min_count=1,\n","  dm=1,\n","  epochs=26,\n","  workers=8\n",")\n","best_doc2vec_model.build_vocab(vdoc_train+vdoc_val)\n","best_doc2vec_model.train(\n","  vdoc_train,\n","  total_examples=best_doc2vec_model.corpus_count,\n","  epochs=best_doc2vec_model.epochs,\n","  queue_factor=16\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJkCJp-CPDlZ"},"source":["best_doc2vec_model.save(models_path + 'doc2vec.d2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2OSrg2DDnqI"},"source":["### Load models"]},{"cell_type":"code","metadata":{"id":"0HzW3vuZuuZx","executionInfo":{"status":"ok","timestamp":1622664831285,"user_tz":-120,"elapsed":1837,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["best_lsa_model = LsiModel.load(models_path + \"lsa_model.lsa\")\n","dictionary = ClippedCorpus.load(models_path + \"lsa_model_dictionary.dic\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsswOnMFRKTG","executionInfo":{"status":"ok","timestamp":1622664833745,"user_tz":-120,"elapsed":2464,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}}},"source":["best_doc2vec_model = Doc2Vec.load(models_path + \"doc2vec.d2v\") "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AIrl244VnaTa"},"source":["###Exact match bonus "]},{"cell_type":"markdown","metadata":{"id":"wxuBo0_9SWSe"},"source":["####Search the best parameters"]},{"cell_type":"code","metadata":{"id":"ke_2aSHfi5Cj"},"source":["print(\"Exact match bonus exploration:\\n\")\n","normalization = False\n","\n","for em_bonus in np.arange(0, 0.35, 0.1):\n","  for ner_em_bonus in np.arange(0, 0.35, 0.1):\n","    print_score(\"Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([lsa_models[best_lsa_param], doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            validation_docs, validation_questions, validation_questions_ner, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YbwDNBRPYnb"},"source":["print(\"Exact match bonus exploration:\\n\")\n","normalization = True\n","\n","for em_bonus in np.arange(0, 0.35, 0.1):\n","  for ner_em_bonus in np.arange(0, 0.35, 0.1):\n","    print_score(\"Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([lsa_models[best_lsa_param], doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            validation_docs, validation_questions[:10], validation_questions_ner[:10], score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OX4bxdkcSZ4y"},"source":["####Best models"]},{"cell_type":"code","metadata":{"id":"TWncj391GPC6"},"source":["em_bonus = 0.2\n","ner_em_bonus = 0.2\n","normalization = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6et3lM07MqZ"},"source":["###Comparison of the models"]},{"cell_type":"markdown","metadata":{"id":"kknhKzxhGgam"},"source":["####LSA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATmb7-6b1bCJ","executionInfo":{"elapsed":123357,"status":"ok","timestamp":1622022858838,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"},"user_tz":-120},"outputId":"cb8df823-c658-4d82-d31d-2afac067d4f5"},"source":["#LSA without bonuses\n","em_bonus = 0\n","ner_em_bonus = 0\n","normalization = False\n","\n","print_score(\"Best LSA, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model], [get_LSA_vector], \n","            test_docs, test_questions, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best LSA, Bonus = 0, Ner_bonus = 0 :\n","Similarity score:  0.7488419794729161 \n","Average position:  5.299173743401423 \n","Standard deviation of positions:  7.329525762679216 \n","Mediana, First 75%/90%/98% documents' position: :  2.0 ,  [ 6. 15. 31.] \n","Ranking score:  0.557485289484027 \n","Target doc as first:  0.41282725505887735 \n","Non valid test samples:  33 \n","Exact match bonus (done, total): (0, 0, 384868) \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6V1R3YUhGeTX"},"source":["#### Dov2Vec"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV0B_odFz-Nn","executionInfo":{"elapsed":212715,"status":"ok","timestamp":1622023071538,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"},"user_tz":-120},"outputId":"734ada71-1e2d-4e6d-82cb-a804d7b64c4a"},"source":["#Doc2Vec without bonuses\n","em_bonus = 0\n","ner_em_bonus = 0\n","normalization = False\n","\n","print_score(\"Best Doc2Vec, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_doc2vec_model], [get_Doc2Vec_vector], \n","            test_docs, test_questions, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best LSA, Bonus = 0, Ner_bonus = 0 :\n","Similarity score:  0.8246534663639014 \n","Average position:  7.423573796730308 \n","Standard deviation of positions:  10.564839294679748 \n","Mediana, First 75%/90%/98% documents' position: :  2.0 ,  [ 9. 24. 41.] \n","Ranking score:  0.5573304164419389 \n","Target doc as first:  0.4483823025037156 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (0, 0, 384868) \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mLZUIXuKEH1X"},"source":["####LSA + Doc2Vec"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGVYMI0zVpoV","executionInfo":{"elapsed":334570,"status":"ok","timestamp":1622022718962,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"},"user_tz":-120},"outputId":"a893f0c1-c042-44d0-d7fa-f72a5998534f"},"source":["#LSA + Doc2Vec without bonuses\n","em_bonus = 0\n","ner_em_bonus = 0\n","normalization = False\n","\n","print_score(\"Best LSA, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model, best_doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            test_docs, test_questions, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best LSA, Bonus = 0, Ner_bonus = 0 :\n","Similarity score:  0.8690171713822491 \n","Average position:  3.987081285012004 \n","Standard deviation of positions:  6.784803531102837 \n","Mediana, First 75%/90%/98% documents' position: :  1.0 ,  [ 3. 11. 30.] \n","Ranking score:  0.6887884447597129 \n","Target doc as first:  0.5732251057505431 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (0, 0, 384868) \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmRzJtzLB_XI","executionInfo":{"elapsed":387197,"status":"ok","timestamp":1622022184539,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"},"user_tz":-120},"outputId":"fd776383-d2e5-418e-9e53-ec4f46346a24"},"source":["#LSA + Doc2Vec with bonuses\n","em_bonus = 0.2\n","ner_em_bonus = 0.2\n","normalization = False\n","\n","print_score(\"Best LSA + Doc2Vec, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model, best_doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            test_docs, test_questions, test_questions_ner, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best LSA + Doc2Vec, Bonus = 0.2, Ner_bonus = 0.2 :\n","Similarity score:  0.9468445855839863 \n","Average position:  2.6630844861095233 \n","Standard deviation of positions:  4.396311345284151 \n","Mediana, First 75%/90%/98% documents' position: :  1.0 ,  [ 2.  6. 18.] \n","Ranking score:  0.7649002064625927 \n","Target doc as first:  0.6598833885903739 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (301001, 88139, 384868) \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pIf6r71qaRNP","executionInfo":{"status":"ok","timestamp":1622633941830,"user_tz":-120,"elapsed":3269027,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"db4c2a14-0a35-4382-8f14-b4a8c0330ca4"},"source":["#LSA + Doc2Vec with bonuses upon all the dataset\n","em_bonus = 0.2\n","ner_em_bonus = 0.2\n","normalization = False\n","\n","print_score(\"Best LSA + Doc2Vec, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model, best_doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            training_docs+validation_docs+test_docs, test_questions, test_questions_ner, \n","            score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best LSA + Doc2Vec, Bonus = 0.2, Ner_bonus = 0.2 :\n","Similarity score:  0.8458246658113037 \n","Average position:  18.0380701954956 \n","Standard deviation of positions:  43.99980814500418 \n","Mediana, First 75%/90%/98% documents' position: :  3.0 ,  [ 10.  48. 176.] \n","Ranking score:  0.4279793900115041 \n","Target doc as first:  0.25140048016462785 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (2948366, 796748, 3831186) \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CsOSZbykHtvY"},"source":["#### Upon paragraphs as docs"]},{"cell_type":"code","metadata":{"id":"KtOWGaWjLE1l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622643442391,"user_tz":-120,"elapsed":2230795,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"adb71140-f74a-47b2-8b9e-75b01eaff751"},"source":["#LSA + Doc2Vec with bonuses, knowing the target doc\n","em_bonus = 0.2\n","ner_em_bonus = 0.2\n","normalization = False\n","\n","print_score(\"Best LSA + Doc2Vec, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model, best_doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            test_docs, test_questions, test_questions_ner, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary, rank_paragraph_from_doc=True)) "],"execution_count":40,"outputs":[{"output_type":"stream","text":["Best LSA + Doc2Vec, Bonus = 0.2, Ner_bonus = 0.2 :\n","Similarity score:  0.9136729301210788 \n","Average position:  1.5725936135998162 \n","Standard deviation of positions:  1.3136011548604263 \n","Mediana, First 75%/90%/98% documents' position: :  1.0 ,  [2. 3. 6.] \n","Ranking score:  0.8388753970077194 \n","Target doc as first:  0.7375100034297474 \n","Non valid test samples:  41 \n","Exact match bonus (done, total): (2116325, 512363, 16759252) \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCignwkaHs57","executionInfo":{"status":"ok","timestamp":1622674159506,"user_tz":-120,"elapsed":9089681,"user":{"displayName":"Andrea Lavista","photoUrl":"","userId":"05251988843513566525"}},"outputId":"0d8c390c-3b1b-41e2-8679-1ddfeeca9c3a"},"source":["#LSA + Doc2Vec with bonuses, searching on all the paragraphs of the test set\n","em_bonus = 0.2\n","ner_em_bonus = 0.2\n","normalization = False\n","\n","print_score(\"Best LSA + Doc2Vec, Bonus = \" + str(em_bonus) + \", Ner_bonus = \" + str(ner_em_bonus), \n","            *evaluate_model([best_lsa_model, best_doc2vec_model], [get_LSA_vector, get_Doc2Vec_vector], \n","            test_docs, test_questions, test_questions_ner, score_normalization=normalization, exact_match_bonus=em_bonus, \n","            ner_exact_match_bonus=ner_em_bonus, dictionary=dictionary)) "],"execution_count":13,"outputs":[{"output_type":"stream","text":["Best LSA + Doc2Vec, Bonus = 0.2, Ner_bonus = 0.2 :\n","Similarity score:  0.9136127635988304 \n","Average position:  15.597347662055562 \n","Standard deviation of positions:  66.16842461451894 \n","Mediana, First 75%/90%/98% documents' position: :  2.0 ,  [  9.  30. 128.] \n","Ranking score:  0.5397222038376489 \n","Target doc as first:  0.42151594832514006 \n","Non valid test samples:  0 \n","Exact match bonus (done, total): (2116325, 512363, 16759252) \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1O0EXbotzZav"},"source":["### Store data"]},{"cell_type":"code","metadata":{"id":"dUtRp4xd7ufs"},"source":["# lsa_docs = compute_documents_vectors([best_lsa_model], [get_LSA_vector], dataset_documents, dictionary=dictionary)\n","# doc2vec_docs = compute_documents_vectors([best_doc2vec_model], [get_Doc2Vec_vector], dataset_documents)\n","\n","# with open('/content/gdrive/My Drive/NLP/Project/SQUAD MATERIAL/docs_lsa.pkl', 'wb') as lsa_output:\n","#   pickle.dump(lsa_docs[0], lsa_output, protocol=pickle.HIGHEST_PROTOCOL)\n","# with open('/content/gdrive/My Drive/NLP/Project/SQUAD MATERIAL/docs_doc2vec.pkl', 'wb') as doc2vec_output:\n","#   pickle.dump(doc2vec_docs[0], doc2vec_output, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbrLKTzrRqlz"},"source":["# paragraphs_data = {\n","#     f'{title}_{par_id}': tuple(\n","#       [ [ token['text'] for token in paragraph ] ] +\n","#       [\n","#         np.array([ token[key] for token in paragraph ])\n","#         for key in ('tag', 'ent_type', 'like_num', 'is_stop')\n","#       ]\n","#     )\n","#     for title, document in preprocessed_documents.items()\n","#     for par_id, paragraph in document.items()\n","# }\n","# with open('/content/gdrive/My Drive/NLP/Project/SQUAD MATERIAL/paragraphs_data.pkl', 'wb') as par_data_out:\n","#   pickle.dump(paragraphs_data, par_data_out, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]}]}