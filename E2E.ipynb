{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"E2E.ipynb","provenance":[],"collapsed_sections":["YSeQY9gSPiRU","FhvWaf9uPl6S","7nXno2MuPqxw","1Cni3YimTBme","SIkeCxx6TGA_","ne0Gbg3rSaev"],"authorship_tag":"ABX9TyMGMGWphwJMq5lJJ7aKHidl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VkTb7SztmvBm"},"source":["%%capture\n","!pip3 install pickle5\n","import re\n","import json\n","import numpy as np\n","import pickle5 as pickle # used for colab\n","from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3-nDAs023_2"},"source":["# E2E Model"]},{"cell_type":"markdown","metadata":{"id":"YSeQY9gSPiRU"},"source":["## Text embedding"]},{"cell_type":"code","metadata":{"id":"QrqVT-_SNsPv"},"source":["class TextEmbedder(object):\n","  '''Interface for document embedding'''\n","  def embed(self, text): raise Exception('Not implemeted!')\n","  def __call__(self, question): return self.embed(question)\n","\n","class LSAModel(TextEmbedder):\n","  '''Wrapper for the Gensim LSA model to be easily used in the E2E model'''\n","  def __init__(self, path, vocabulary):\n","    from gensim.models import LsiModel\n","    from gensim.utils import ClippedCorpus\n","    self.model = LsiModel.load(path)\n","    self.frequency_threshold = frequency_threshold\n","    self.vocabulary = ClippedCorpus.load(vocabulary)\n","\n","  def embed(self, text): return [ v for d, v in self.model[self.vocabulary.doc2bow(text)] ]\n","\n","class Doc2VecModel(TextEmbedder):\n","  '''Wrapper for the Gensim Doc2Vec model to be easily used in E2E model'''\n","  def __init__(self, path, epochs=200):\n","    from gensim.models.doc2vec import Doc2Vec\n","    self.model = Doc2Vec.load(path) \n","    self.epochs = epochs\n","  \n","  def embed(self, text): return self.model.infer_vector(text, epochs=self.epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhvWaf9uPl6S"},"source":["## Question Answering"]},{"cell_type":"code","metadata":{"id":"za25ysFlPud2"},"source":["class TextPreprocessor(object):\n","  '''Interface for lemmatization and ner tagging'''\n","  def lemmatize(self, question): raise Exception('Not implemented!')\n","  def ner(self, question): raise Exception('Not implemented!')\n","\n","class QuestionAnswer(TextPreprocessor):\n","  '''Interface for Question Answering task'''\n","  def answer(self, question, paragraph): raise Exception('Not implemented!')\n","  def __call__(self, question, paragraph): return self.answer(question, paragraph)\n","\n","class QAModel(QuestionAnswer):\n","  '''Wrapper for the Question Answer neural model to be easily used in the E2E model'''\n","  # models cache\n","  MODELS = dict()\n","\n","  def __init__(self, path, tokenizer_path, ner_path, pos_path, question_padding=60, paragraph_padding=400):\n","    from tensorflow.keras.models import load_model\n","    from tensorflow.keras.utils import to_categorical\n","    import en_core_web_sm\n","    import pickle5 as pickle\n","    \n","    # if the path is not in the cache load the model from the file system\n","    if path not in QAModel.MODELS: QAModel.MODELS[path] = load_model(path)\n","\n","    # load all the model dependencies\n","    self.model = QAModel.MODELS[path]\n","    self.preprocessor = en_core_web_sm.load()\n","    with open(tokenizer_path, 'rb') as handle: self.tokenizer = pickle.load(handle)\n","    with open(ner_path, 'rb') as handle: self.ner_codes = pickle.load(handle)\n","    with open(pos_path, 'rb') as handle: self.pos_codes = pickle.load(handle)\n","    self.question_padding = question_padding\n","    self.paragraph_padding = paragraph_padding\n","    self.question_keys = ['what','how','why','where','when','which','who','whose','whom']\n","\n","  def lemmatize(self, question):\n","    return [\n","      token.text\n","      for token in self.preprocessor(question)\n","      if not (token.is_punct or token.is_stop)\n","    ]\n","\n","  def ner(self, question):\n","    return [\n","      token.ent_type_\n","      for token in self.preprocessor(question)\n","      if not (token.is_punct or token.is_stop)\n","    ]\n","\n","  def __tokenize(self, word):\n","    '''uses the tokenizer dependency to convert a word into a numerical token'''\n","    r = self.tokenizer.texts_to_sequences([ word ])[0]\n","    if len(r): return r[0]\n","    return 0\n","\n","  def __pad(self, tokens, length, padding=0):\n","    '''computes the pre padding (faster then tensorflow)'''\n","    return np.array([ padding ] * (length - len(tokens)) + list(tokens)[:length])\n","\n","  def __preprocess(self, text, padding=None):\n","    '''preprocess a text converting it in an input suitable for the model'''\n","    tokens = self.preprocessor(text)\n","    raw = []\n","    texts = []\n","    tags  = []\n","    ents  = []\n","    nums  = []\n","    stops = []\n","    for token in tokens:\n","      if token.is_punct: continue\n","      raw.append(token.text)\n","      texts.append(self.__tokenize(token.text))\n","      tags.append(self.pos_codes.get(token.tag_, 0))\n","      ents.append(self.ner_codes.get(token.ent_type_, 0))\n","      nums.append(token.like_num)\n","      stops.append(token.is_stop)\n","\n","    features = [ self.__pad(feature, length=padding) for feature in (texts, tags, ents, nums, stops) ]\n","    features = [ feature.reshape((1,) + feature.shape + (1, )) for feature in features ]\n","    return tuple(features), raw\n","\n","  def __presence_flag(self, question_tokens, paragraph_tokens):\n","    '''\n","    creates an array of flags where 1 means that a paragraph token is\n","    present in the question, while 0 means that is absent \n","    '''\n","    question_tokens = list(question_tokens.squeeze())\n","    paragraph_tokens = list(paragraph_tokens.squeeze())\n","    result = np.array([ bool(token and token in question_tokens) for token in paragraph_tokens ])\n","    return result.reshape((1, ) + result.shape + (1, ))\n","  \n","  def __classify_question(self, question):\n","    '''\n","    computes the classification id of a question.\n","    If the question contains only one keyword (like 'what', 'where', etc.)\n","    then the id is assigned. If no keywords or more then one are found, then\n","    the classification outputs the 0 id (absent).    \n","    '''\n","    result = 0\n","    found = False\n","    for key_id, key in enumerate(self.question_keys):\n","      if key in question:\n","        if found:\n","          result = 0\n","          break\n","        else:\n","          found = True\n","          result = key_id + 1\n","    return np.array([[ result ]])\n","\n","  def answer(self, question, paragraph):\n","    '''computes the answer given a question and a paragraph'''\n","    pre_qst, _ = self.__preprocess(question, padding=self.question_padding)\n","    qst_class = self.__classify_question(question)\n","\n","    pre_par, raw_par = self.__preprocess(paragraph, padding=self.paragraph_padding)\n","    \n","    # shift is needed to align padded output and start/end indexes \n","    shift = max(self.paragraph_padding - len(raw_par), 0)\n","\n","    X = dict(\n","      input_question=pre_qst[0].reshape(pre_qst[0].shape[:-1]),\n","      input_question_pos_tag=pre_qst[1],\n","      input_question_ner_tag=pre_qst[2],\n","      input_question_is_num=pre_qst[3],\n","      input_question_is_stop=pre_qst[4],\n","      input_question_class=qst_class,\n","\n","      input_paragraph=pre_par[0],\n","      input_paragraph_pos_tag=pre_par[1],\n","      input_paragraph_ner_tag=pre_par[2],\n","      input_paragraph_is_num=pre_par[3],\n","      input_paragraph_is_stop=pre_par[4],\n","      input_paragraph_flag=self.__presence_flag(pre_qst[0], pre_par[0])\n","    )\n","    Y = self.model.predict(X)\n","\n","    # extract start and end from probabilities and align with the shift\n","    start, end = np.argmax(Y[0], axis=-1).item(), np.argmax(Y[1], axis=-1).item()\n","    start_offset, end_offset = start - shift, end - shift + 1\n","\n","    return (start_offset, end_offset), (Y[0][0][start], Y[1][0][end]), str.join(' ', raw_par[start_offset:end_offset])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nXno2MuPqxw"},"source":["## E2E Wrapper"]},{"cell_type":"markdown","metadata":{"id":"1Cni3YimTBme"},"source":["### IR Utilities"]},{"cell_type":"code","metadata":{"id":"rtUdsgbeb43l"},"source":["#infer vector from Doc2Vec model\n","def get_Doc2Vec_vector(model, text, **kwargs):\n","  return model.infer_vector(text, epochs=kwargs.get('doc2vec_infer_epochs', 200))\n","\n","#infer vector from LSA model\n","def get_LSA_vector(model, text, **kwargs):\n","  return [v for d,v in model[kwargs['dictionary'].doc2bow(text)]]\n","\n","#compute similarity using sklearn library\n","def cosine_similarity(question_vector, doc_vector):\n","  return sk_cosine_similarity([question_vector], [doc_vector]).item()\n","\n","def sort_dict_by_value(dictionary, reverse=True):\n","  return {k: v for k, v in sorted(dictionary.items(), reverse=reverse, key=lambda item: item[1])}\n","\n","#check if one of the two vectors has dimension 0\n","def not_null(vector1, vector2):\n","  if type(vector1) != tuple:\n","    return len(vector1)*len(vector2)\n","  for i in range(len(vector1)):\n","    if not len(vector1[i])*len(vector2[i]):\n","      return False\n","  return True\n","\n","def is_word_in_doc(word, doc):\n","  return word in doc\n","\n","def is_question_word_in_doc(question, doc):\n","  for word in question:\n","    if is_word_in_doc(word, doc):\n","      return True\n","  return False\n","\n","#select the tokens inside the param text that are tagged with a NER tag\n","def select_ner_tokens(text, ner_tagged_text):\n","  entity_tokens = []\n","  for token, ner_tag in zip(text, ner_tagged_text):\n","    if ner_tag != 0:\n","      entity_tokens.append(token)\n","  return entity_tokens\n","\n","#compute with the 'models' the vectors of the 'documents' using the functions 'get_vector_functions'\n","def compute_documents_vectors(models, get_vector_functions, documents, **kwargs):\n","  doc_vectors = []\n","  model_scores = []\n","  for i,model in enumerate(models):\n","    doc_vectors.append(dict())\n","    #compute document vectors\n","    for title,doc in documents:\n","      doc_vectors[i][title] = (doc, get_vector_functions[i](model, doc, **kwargs))\n","  return doc_vectors\n","\n","#compute the similarity for each of the doc-question pairs\n","def compute_scores_by_vectors(doc_vectors, question_vector):\n","  scores = dict()\n","  #for title,(_,doc_vector) in tqdm.tqdm(doc_vectors.items()):\n","  for title,(_,doc_vector) in doc_vectors.items():\n","    #compute the score according to the cosine similarity received as parameter \n","    scores[title] = cosine_similarity(question_vector, doc_vector) if not_null(question_vector, doc_vector) else 0\n","  return scores\n","\n","#return the documents with the scores, sorted by the score\n","def compute_documents_ranking(models, get_vector_functions, doc_vectors, question, question_ner=None, score_normalization=True, exact_match_bonus=0, ner_exact_match_bonus=0, **kwargs):\n","  #definition of variables\n","  model_scores = []\n","  em_bonus_counter = 0\n","  ner_em_bonus_counter = 0\n","  \n","  #compute question vector and score between question and docs\n","  for i,model in enumerate(models):\n","    question_vector = get_vector_functions[i](model, question, **kwargs)\n","    model_scores.append(compute_scores_by_vectors(doc_vectors[i], question_vector))   \n","  #perform the normalization of the scores\n","  if score_normalization:\n","    for i in range(len(model_scores)):\n","      model_scores[i] = normalize_dict_values(model_scores[i])\n","\n","  #average scores if more models are passed\n","  scores = dict()\n","  for title in model_scores[0].keys():\n","    score_sum = 0\n","    for i in range(len(model_scores)):\n","      score_sum += model_scores[i][title]\n","    scores[title] = score_sum / len(model_scores)\n","\n","  #if the question vector is null, then all the scores will be equal to 0. If this condition doesn't hold compute the exact match's bonuses\n","  if not (min(scores.values()) == 0 and max(scores.values()) == 0):\n","    #compute exact match bonus\n","    if exact_match_bonus or ner_exact_match_bonus:\n","      #select tokens in the question with a NER tag assigned \n","      if question_ner and ner_exact_match_bonus:\n","        ner_tokens = select_ner_tokens(question, question_ner)\n","      for title,(doc,_) in doc_vectors[0].items():\n","        #add score bonus based on exact match if the doc contains the the words of the question\n","        keyword_present = is_question_word_in_doc(question, doc) if exact_match_bonus else False\n","        ner_keyword_present = is_question_word_in_doc(ner_tokens, doc) if question_ner and ner_exact_match_bonus else False\n","        #add exact match bonuses, if present, without exceed 1\n","        scores[title] = scores[title] + (exact_match_bonus * keyword_present) + (ner_exact_match_bonus * ner_keyword_present)\n","        #count how many pairs question-docs are affected by the exact match bonus\n","        if keyword_present:\n","          em_bonus_counter += 1\n","        if ner_keyword_present:\n","          ner_em_bonus_counter += 1\n","  #else:\n","    #print(\"Question: \", question, \"\\n\")\n","  ranked_docs = sort_dict_by_value(normalize_dict_values(scores))\n","  return ranked_docs, em_bonus_counter, ner_em_bonus_counter\n","\n","def normalize_dict_values(dictionary):\n","  #if all the values are not equal, perform the normalization\n","  #find min and max\n","  min_value = min(dictionary.values())\n","  max_value = max(dictionary.values())\n","  if min_value != max_value: \n","    #normalize: (value - min) / max - min\n","    for k,v in dictionary.items():\n","      dictionary[k] = (dictionary[k] - min_value) / (max_value - min_value)\n","  else:\n","    dictionary = {k:1 for k,_ in dictionary.items()}\n","  return dictionary\n","\n","#return the results of the metrics for a specific model (or a set of models combined together)\n","def evaluate_model(models, get_vector_functions, documents, questions, questions_ner=None, score_normalization=True, exact_match_bonus=0, ner_exact_match_bonus=0, **kwargs):\n","  #definition of variables\n","  similarity_score = []\n","  ranking_score = []\n","  positions = []\n","  target_doc_first = 0\n","  num_questions = 0\n","  \n","  #var used for debugging and for understaind better some behaviours\n","  em_bonus_counter = 0\n","  ner_em_bonus_counter = 0\n","  non_valid_samples = 0\n","  \n","  #compute the document vectors\n","  doc_vectors = compute_documents_vectors(models, get_vector_functions, documents, **kwargs)\n","\n","  #compute the doc-question scores\n","  ner_tagged_question = None\n","  #for (target_doc, doc_questions) in tqdm.tqdm(questions):\n","  for (target_doc, doc_questions) in questions:\n","    #check if EM_NER_bonus is enabled\n","    if questions_ner and ner_em_bonus:\n","      doc_questions_ner = [doc_questions_ner for doc_title,doc_questions_ner in questions_ner if doc_title==target_doc][0]\n","    #for each questions compute the ranking of the docs and update the metrics\n","    for i,question in enumerate(doc_questions):\n","      if questions_ner and ner_em_bonus:\n","        ner_tagged_question = doc_questions_ner[i]\n","      num_questions += 1\n","      #compute documents ranking\n","      ranked_docs, exact_match_bonuses, ner_exact_match_bonuses = compute_documents_ranking(models, get_vector_functions, doc_vectors, question, ner_tagged_question, score_normalization, exact_match_bonus, ner_exact_match_bonus, **kwargs)\n","      #update a counter to understand how many doc-questions paris are affected by the bonuses\n","      em_bonus_counter += exact_match_bonuses\n","      ner_em_bonus_counter += ner_exact_match_bonuses\n","\n","      #When testing paragraph serach knowing the target document, removing the paragraphs of the others paragraphs \n","      if kwargs.get('rank_paragraph_from_doc', False):\n","        ranked_docs = sort_dict_by_value({k:v for k,v in ranked_docs.items() if k[:k.rfind('-')] == target_doc[:target_doc.rfind('-')]})\n","\n","      #some questions in the dataset are mapped in a null vector: in that case the documents' scores are all equal to 0 \n","      if min(ranked_docs.values()) == max(ranked_docs.values()): \n","        #just a counte for debugging\n","        non_valid_samples += 1\n","      else:\n","        #compute metrics\n","        similarity_score.append(ranked_docs[target_doc])\n","        docs_positions = {k:(i+1) for i,(k,v) in enumerate(ranked_docs.items())}\n","        ranking_score.append(1/docs_positions[target_doc])\n","        positions.append(docs_positions[target_doc])\n","        if (docs_positions[target_doc] == 1):\n","          target_doc_first += 1      \n","\n","  return similarity_score, ranking_score, positions, target_doc_first/num_questions, non_valid_samples, (em_bonus_counter, ner_em_bonus_counter, len(documents*num_questions))\n","\n","def print_score(model_name, similarity_score, ranking_score, position_score, first_score, non_valid_samples, em_multiplier_counter):\n","  print(model_name, \":\\nSimilarity score: \", np.mean(similarity_score), \"\\nAverage position: \", np.mean(position_score), \n","        \"\\nStandard deviation of positions: \", np.std(position_score), \n","        \"\\nMediana, First 75%/90%/98% documents' position: : \", np.median(position_score), \", \", np.quantile(position_score, [0.75, 0.90, 0.98]), \"\\nRanking score: \", np.mean(ranking_score), \"\\nTarget doc as first: \", first_score, \"\\nNon valid test samples: \", non_valid_samples, \"\\nExact match bonus (done, total):\", em_multiplier_counter, \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIkeCxx6TGA_"},"source":["### E2E Model"]},{"cell_type":"code","metadata":{"id":"OdDQSA4Gm5yj"},"source":["class E2E:\n","  def __init__(\n","    self,\n","    answer_model,\n","    preprocessor,\n","    models=[],\n","    documents=[],\n","    paragraphs=[],\n","    paragraph_texts=None,\n","    n_docs=None,\n","    n_pars=None\n","  ):\n","    '''\n","    Model that wrap IR and QA models:\n","    qa_model - a function that takes question and paragraph and returns start and end\n","                qa_model(question, paragraph) -> (start, end)\n","    models - a list of functions that take a string and return a vector of fixed size\n","                models[i](text) -> np.array[100]\n","    documents - a list of paths where to load vectors, alternatively a list of\n","                dictionaries of the following shape:\n","    paragraphs - a list of paths where to load vectors, alternatively a list of\n","                 dictionaries of the same shape as documents.\n","    titles - a list of titles to consider when searching in the documents (used for test purposes) \n","    paragraph_texts - a dictionary of all possible paragraphs - { paragraph_id: paragraph_text } \n","    n_best_docs - default number of documents to keep during question-document comparison\n","    n_best_pars - default number of paragraph to keep during question-paragraph comparison\n","    '''\n","    self.answer_model = answer_model\n","    self.preprocessor = preprocessor\n","    self.models = models\n","    self.documents = []\n","    self.paragraphs = []\n","    self.paragraph_texts = dict()\n","    self.n_best_docs = n_docs\n","    self.n_best_pars = n_pars\n","\n","    # load documents\n","    for document in documents:\n","      if type(document) is str: self.load_documents(document)\n","      else: self.documents.append(document)\n","    \n","    # load paragraphs\n","    for paragraph in paragraphs:\n","      if type(paragraph) is str: self.load_paragraphs(paragraph)\n","      else: self.documents.append(paragraph)\n","\n","    # load paragraph texts\n","    if type(paragraph_texts) is str:\n","      self.load_paragraph_texts(paragraph_texts)\n","    elif type(paragraph_texts) is dict:\n","      self.paragraph_texts.update(paragraph_texts)\n","    elif type(paragraph_texts) in [list, tuple]:\n","      self.paragraph_texts.update({ title: value for title, value in paragraph_texts })\n","\n","  def __read_file(self, path):\n","    with open(path, 'rb') as handle:\n","      result = pickle.load(handle)\n","    return result\n","  \n","  def load_paragraph_texts(self, path):\n","    self.paragraph_texts.update(self.__read_file(path))\n","    return self\n","  \n","  def load_paragraphs(self, path):\n","    self.paragraphs.append(self.__read_file(path))\n","    return self\n","\n","  def load_documents(self, path):\n","    self.documents.append(self.__read_file(path))\n","    return self\n","\n","  def predict(self, questions, n_docs=None, n_pars=None, doc_norm=False, par_norm=False):\n","    '''\n","    Given a list of questions returns a list of predictions, matching the first\n","    prediction available from question answer model:\n","    - extract the best paragraphs matching each question\n","    - find start and end with the question answer model\n","\n","    The result is a list of the same size of the questions input, with this shape:\n","    [ ((start, end), (start_confidence, end_confidence), answer, paragraph), ... ]\n","    '''\n","    if n_docs is None: n_docs = self.n_best_docs\n","    if n_pars is None: n_pars = self.n_best_pars\n","\n","    if n_docs < 0: n_docs = None\n","    if n_pars < 0: n_pars = None\n","\n","    # if just one question is passed, return the output as a flat answer\n","    flat_return = type(questions) is str\n","    if flat_return: questions = [ questions ]\n","\n","    result = []\n","    for question in questions:\n","\n","      # question preprocessing\n","      question_lemmatized = self.preprocessor.lemmatize(question)\n","      question_ner = self.preprocessor.ner(question)\n","      # document ranking\n","      best_docs, _, _ = compute_documents_ranking(\n","        models=self.models,\n","        get_vector_functions=[ lambda m, q: m(q) for model in self.models ],\n","        doc_vectors=self.documents,\n","        question=question_lemmatized,\n","        question_ner=question_ner,\n","        score_normalization=doc_norm,\n","        exact_match_bonus=0.2,\n","        ner_exact_match_bonus=0.2\n","      )\n","      # take just the n_docs best documents\n","      best_docs = list(best_docs.items())[:n_docs]\n","      best_docs_titles = [ title for title, _ in best_docs ]\n","\n","      # paragraphs extraction\n","      best_docs_pars = [\n","        { t: v for t, v in model_pars.items() if re.sub(r'_\\d+$', '', t) in best_docs_titles }\n","        for model_pars in self.paragraphs\n","      ]\n","\n","      # paragraph ranking\n","      best_pars, _, _ = compute_documents_ranking(\n","        models=self.models,\n","        get_vector_functions=[ lambda m, q: m(q) for model in self.models ],\n","        doc_vectors=best_docs_pars,\n","        question=question_lemmatized,\n","        question_ner=question_ner,\n","        score_normalization=par_norm,\n","        exact_match_bonus=0.2,\n","        ner_exact_match_bonus=0.2\n","      )\n","      best_par_ids = list(best_pars)[:n_pars]\n","      best_pars_text = [\n","        self.paragraph_texts[paragraph_id]\n","        for paragraph_id in best_par_ids\n","      ]\n","      \n","      # question answering\n","      values = []\n","      for paragraph_text in best_pars_text:\n","        (s, e), (s_p, e_p), text = self.answer_model(question, paragraph_text)\n","        values.append(((s, e), (s_p, e_p), text, paragraph_text))\n","      \n","      # sort the answers by the sum of the confidence scores\n","      # given by the question answering system\n","      values = list(sorted(values, key=lambda v: -sum(v[1])))\n","      result.append(values)\n","\n","    return result[0] if flat_return else result\n","\n","  def ask(self, questions, n_docs=None, n_pars=None, doc_norm=False, par_norm=False):\n","    '''\n","    Given a list of questions returns a list of answers, matching the first\n","    prediction available from question answer model:\n","    - extract the best paragraphs matching each question\n","    - find start and end with the question answer model\n","    - extract the answer from the predictions\n","\n","    The result is a list of textual answers\n","    '''\n","    # if just one question is passed, return the output as a flat answer\n","    flat_return = type(questions) is str\n","    if flat_return: questions = [ questions ] \n","    predictions = self.predict(questions, n_docs, n_pars, doc_norm, par_norm)\n","    # output alignment with the flat return\n","    return predictions[0][0][2] if flat_return else [ pred[0][2] for pred in predictions ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8VBVezZaad9"},"source":["# E2E Evaluation"]},{"cell_type":"code","metadata":{"id":"1I7LG50qyy73"},"source":["# from google.colab import drive, files\n","# drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7hya6mkSJuJ"},"source":["root_path = '/content/gdrive/My Drive/NLP/Project'\n","squad_path = f'{root_path}/SQUAD MATERIAL'\n","model_path = f'{root_path}/models'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKkSx9UPaePd"},"source":["d2v_model = Doc2VecModel(f'{model_path}/doc2vec.d2v')\n","d2v_docs = f'{squad_path}/docs_doc2vec.pkl'\n","d2v_pars = f'{squad_path}/paragraphs_doc2vec.pkl'\n"," \n","lsa_model = LSAModel(\n","  f'{model_path}/lsa_model.lsa',\n","  f'{model_path}/lsa_model_dictionary.dic'\n",")\n","lsa_docs = f'{squad_path}/docs_lsa.pkl'\n","lsa_pars = f'{squad_path}/paragraphs_lsa.pkl'\n"," \n","par_texts = f'{squad_path}/paragraph_texts.pkl'\n"," \n","qas_model = QAModel(\n","  path=f'{model_path}/squad-model.h5',\n","  tokenizer_path=f'{squad_path}/word_tokenizer.pkl',\n","  ner_path=f'{squad_path}/ner_tokenizer.pkl',\n","  pos_path=f'{squad_path}/pos_tokenizer.pkl'\n",")\n"," \n","e2e = E2E(\n","  answer_model=qas_model,\n","  preprocessor=qas_model,\n","  models=[ d2v_model, lsa_model ],\n","  documents=[ d2v_docs, lsa_docs ],\n","  paragraphs=[ d2v_pars, lsa_pars ],\n","  paragraph_texts=par_texts,\n","  n_docs=10,\n","  n_pars=10\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ne0Gbg3rSaev"},"source":["### E2E Testing"]},{"cell_type":"code","metadata":{"id":"xZKRKPB0JKdo"},"source":["with open(f'{squad_path}/titles_split.pkl', 'rb') as f: titles_split = pickle.load(f)\n","with open(f'{squad_path}/training_set.json', 'r') as f: dataset = json.load(f)\n","questions = [\n","  (question['id'], question['question'], question['answers'][0]['text'], paragraph['context'], document['title'], str.join(' ', (p['context'] for p in document['paragraphs'])) )\n","  for document in dataset['data']\n","  for paragraph in document['paragraphs']\n","  for question in paragraph['qas']\n","  if document['title'] in titles_split['test']\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2GD7QuQMHj4"},"source":["import tqdm.autonotebook as tqdm\n","import ipywidgets as pyw\n","from IPython.display import display, clear_output\n"," \n","displayer = pyw.Output()\n","\n","N_DOCS = 20\n","N_PARS = 1\n","slash_n = '\\n'\n"," \n","first_exact = 0\n","first_contained = 0\n","exact = 0\n","contained = 0\n","catched_pars = 0\n","catched_docs = 0\n","\n","count = 0\n","\n","exact_avg = (exact / count) if exact and count else -1\n","contained_avg = (contained / count) if exact and count else -1\n","catched_pars_avg = (catched_pars / count) if exact and count else -1\n","catched_docs_avg = (catched_docs / count) if exact and count else -1\n","\n","exact_matches = []\n","contained_matches = []\n","\n","total = len(questions)\n","with tqdm.tqdm(questions[count:]) as bar:\n","  \n","  display(displayer)\n","\n","  for id, question, right_answer, right_par, right_title, right_doc in bar:\n","    \n","    predictions = e2e.predict(\n","      question,\n","      n_docs=N_DOCS,\n","      n_pars=N_PARS\n","    )\n","    \n","    r = 'BEST OPTION - ' + predictions[0][2]\n","\n","    for position, ((start, end), (start_prob, end_prob), guess_answer, guess_par) in enumerate(predictions):\n","      position += 1\n","      if guess_par in right_doc:\n","        catched_docs_avg = (catched_docs_avg * catched_docs + position) / (catched_docs + 1)\n","        catched_docs += 1\n","        break\n","\n","    for position, ((start, end), (start_prob, end_prob), guess_answer, guess_par) in enumerate(predictions):\n","      position += 1\n","      if guess_par == right_par:\n","        catched_pars_avg = (catched_pars_avg * catched_pars + position) / (catched_pars + 1)\n","        catched_pars += 1\n","        break\n","\n","    for position, ((start, end), (start_prob, end_prob), guess_answer, guess_par) in enumerate(predictions):\n","      position += 1\n","      if guess_answer == right_answer:\n","        exact_avg = (exact_avg * exact + position) / (exact + 1)\n","        exact += 1\n","        if position == 1: first_exact += 1 \n","        r = 'EXACT - ' + guess_answer\n","        exact_matches.append(f'\\t{question} - {guess_answer}')\n","        break\n","\n","      if guess_answer and (guess_answer in right_answer or right_answer in guess_answer):\n","        contained_avg = (contained_avg * contained + position) / (contained + 1)\n","        contained += 1\n","        if position == 0: first_contained += 1\n","        r = 'CATCH - ' + guess_answer\n","        contained_matches.append(f'\\t{question} - {guess_answer} ({right_answer})')\n","        break\n","\n","    count += 1\n","    with displayer:\n","      clear_output()\n","      print(f'''--- STATS ---\n","\n","EM FIRST: {first_exact} ({first_exact / count:.2%})\n","CM FIRST: {first_contained} ({first_contained / count:.2%})\n","\n","EM: {exact} ({exact / count:.2%}) - {exact_avg} avg position\n","CM: {contained} ({contained / count:.2%}) - {contained_avg} avg position\n","\n","CATCHED DOCUMENTS: {catched_docs} ({catched_docs / count:.2%}) - {catched_docs_avg} avg position\n","CATCHED PARAGRAPHS: {catched_pars} ({catched_pars / count:.2%}) - {catched_pars_avg} avg position\n","\n","COUNT: {count} / {total}\n","\n","QUESTION: {question}\n","  MATCH:  {r}\n","  ANSWER: {right_answer}\n","\n","-----------\n","\n","EXACT MATCHES:\n","{slash_n.join(str(x) for x in exact_matches)}\n","\n","-----------\n","\n","CONTAINED MATCHES:\n","{slash_n.join(str(x) for x in contained_matches)}\n","\n","''')"],"execution_count":null,"outputs":[]}]}